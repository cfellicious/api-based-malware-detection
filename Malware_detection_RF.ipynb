{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca7cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from utils.constants import necessary_keys\n",
    "from utils.categorical_encoding import generate_length_based_categorical_encoding\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from utils.utils import get_train_test_files, get_label_mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61e76c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the metrics\n",
    "def print_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    f1 = f1_score(y_true=y_true, y_pred=y_pred)\n",
    "    precision = precision_score(y_true=y_true, y_pred=y_pred)\n",
    "    recall = recall_score(y_true=y_true, y_pred=y_pred)\n",
    "    print('Accuracy:', accuracy)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1-Score:', f1)\n",
    "\n",
    "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "176dda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(client, inverse_mapping, train_files, test_files, label_dict, max_label, call_dict, train_size, test_size,\n",
    "         single_label_only, length, bigram_info, trigram_info):\n",
    "\n",
    "\n",
    "    # Generate the unigram, bigram, and trigram vectors based on sliding window\n",
    "    unigram_dataset, bigram_dataset, trigram_dataset, labels = (generate_length_based_categorical_encoding\n",
    "                                                                (files=train_files, label_dict=label_dict,\n",
    "                                                                 client=client, max_label=max_label,\n",
    "                                                                 keys=necessary_keys, call_dict=call_dict,\n",
    "                                                                 bigram_info=bigram_info, trigram_info=trigram_info,\n",
    "                                                                 block_size=train_size, length=length))\n",
    "\n",
    "    combined_dataset = np.hstack((unigram_dataset, bigram_dataset, trigram_dataset))\n",
    "    malware_benign_labels = [1 if inverse_mapping.get(x) == 'benign' else 0 for x in labels]\n",
    "    clf_unigram = RandomForestClassifier()\n",
    "    clf_bigram = RandomForestClassifier()\n",
    "    clf_trigram = RandomForestClassifier()\n",
    "    clf_combined = RandomForestClassifier()\n",
    "\n",
    "    clf_unigram.fit(X=unigram_dataset, y=malware_benign_labels)\n",
    "    clf_bigram.fit(X=bigram_dataset, y=malware_benign_labels)\n",
    "    clf_trigram.fit(X=trigram_dataset, y=malware_benign_labels)\n",
    "    clf_combined.fit(X=combined_dataset, y=malware_benign_labels)\n",
    "\n",
    "    test_unigram_X, test_bigram_X, test_trigram_X, test_y = (\n",
    "        generate_length_based_categorical_encoding(files=test_files, label_dict=label_dict, client=client,\n",
    "                                                   max_label=max_label, keys=necessary_keys,\n",
    "                                                   call_dict=call_dict, block_size=test_size,\n",
    "                                                   single_label=single_label_only, length=length,\n",
    "                                                   bigram_info=bigram_info, trigram_info=trigram_info))\n",
    "\n",
    "    malware_benign_test_labels = [1 if inverse_mapping.get(x) == 'benign' else 0 for x in test_y]\n",
    "    test_combined_dataset = np.hstack((test_unigram_X, test_bigram_X, test_trigram_X))\n",
    "    print('\\n-------------------------------------------------------------------------------------')\n",
    "    print('Length:', str(length))\n",
    "\n",
    "    y_pred = clf_unigram.predict(test_unigram_X)\n",
    "    print('\\nUnigram Metrics')\n",
    "    print_metrics(y_true=malware_benign_test_labels, y_pred=y_pred)\n",
    "\n",
    "    y_pred = clf_bigram.predict(test_bigram_X)\n",
    "    print('\\nBigram Metrics')\n",
    "    print_metrics(y_true=malware_benign_test_labels, y_pred=y_pred)\n",
    "\n",
    "    y_pred = clf_trigram.predict(test_trigram_X)\n",
    "    print('\\nTrigram Metrics')\n",
    "    print_metrics(y_true=malware_benign_test_labels, y_pred=y_pred)\n",
    "\n",
    "    y_pred = clf_combined.predict(test_combined_dataset)\n",
    "    print('\\nCombined Metrics')\n",
    "    print_metrics(y_true=malware_benign_test_labels, y_pred=y_pred)\n",
    "    print('\\n-------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c07449af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mappings from label names to numbers and vice versa\n",
    "label_dict, max_label, inverse_mapping = get_label_mapping_dict()\n",
    "with open('function_encoding.json', 'r') as fp:\n",
    "    call_dict = json.load(fp=fp)\n",
    "\n",
    "# Get the bigram infomation such as the bigram count and index of individual bigrams in the feature vector\n",
    "with open('bigram_info.json', 'r') as fp:\n",
    "    bigram_info = json.load(fp=fp)\n",
    "\n",
    "# Get the trigram infomation such as the trigram count and index of individual trigrams in the feature vector\n",
    "with open('trigram_info.json', 'r') as fp:\n",
    "    trigram_info = json.load(fp=fp)\n",
    "\n",
    "# Test percentage\n",
    "test_percentage = 0.25\n",
    "train_size = 100000\n",
    "\n",
    "# This test size is the number of files selected from the test_files. \n",
    "# If it is larger then all test files are considered.\n",
    "test_size = int(train_size * test_percentage)\n",
    "\n",
    "# For reproducibility\n",
    "random_state = 42\n",
    "\n",
    "# For using with Minio client. For this we use only local files, as we downloaded the dataset locally.\n",
    "use_local_files_only = True\n",
    "\n",
    "# Use multiclass predictions. \n",
    "# The number of multiclass labels is very low, therefore we treat it as a single class task.\n",
    "single_label_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd525ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Files:334044\n"
     ]
    }
   ],
   "source": [
    "train_files, test_files = get_train_test_files(client=None, train_files_count=train_size,\n",
    "                                               test_file_percentage=test_percentage, random_state=random_state,\n",
    "                                               use_local_files_only=use_local_files_only,\n",
    "                                               single_label_only=single_label_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7502f790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files: 100%|██████████████████████████| 100000/100000 [1:06:21<00:00, 25.12it/s]\n",
      "Files: 100%|██████████████████████████████| 25000/25000 [16:24<00:00, 25.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------------\n",
      "Length: 5000\n",
      "\n",
      "Unigram Metrics\n",
      "Accuracy: 0.9926\n",
      "Precision: 0.9277864992150706\n",
      "Recall: 0.8095890410958904\n",
      "F1-Score: 0.8646671543525969\n",
      "[[24224    46]\n",
      " [  139   591]]\n",
      "\n",
      "Bigram Metrics\n",
      "Accuracy: 0.99212\n",
      "Precision: 0.9434276206322796\n",
      "Recall: 0.7767123287671233\n",
      "F1-Score: 0.8519909842223893\n",
      "[[24236    34]\n",
      " [  163   567]]\n",
      "\n",
      "Trigram Metrics\n",
      "Accuracy: 0.98272\n",
      "Precision: 0.8706467661691543\n",
      "Recall: 0.4794520547945205\n",
      "F1-Score: 0.6183745583038869\n",
      "[[24218    52]\n",
      " [  380   350]]\n",
      "\n",
      "Combined Metrics\n",
      "Accuracy: 0.9912\n",
      "Precision: 0.9351535836177475\n",
      "Recall: 0.7506849315068493\n",
      "F1-Score: 0.8328267477203648\n",
      "[[24232    38]\n",
      " [  182   548]]\n",
      "\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lengths = [50, 100, 150, 200, 250, 500, 750, 1000, 2500, 5000, 7500, 10000, 20000]\n",
    "lengths = [5000]\n",
    "# Test the method for different lengths of API calls.\n",
    "for length in lengths:\n",
    "    test(client=None, inverse_mapping=inverse_mapping, train_files=train_files, test_files=test_files,\n",
    "         label_dict=label_dict, max_label=max_label, call_dict=call_dict, train_size=train_size,\n",
    "         test_size=test_size, single_label_only=single_label_only, length=length, bigram_info=bigram_info,\n",
    "         trigram_info=trigram_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501d713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:api_call] *",
   "language": "python",
   "name": "conda-env-api_call-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
